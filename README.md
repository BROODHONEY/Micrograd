# Micrograd from Scratch (Jupyter Notebooks)

This repository contains my **from-scratch implementation of `micrograd`**, a tiny automatic differentiation engine originally created by **Andrej Karpathy**.

I built this project by following and deeply understanding Andrej Karpathy’s YouTube lecture, and then **re-implementing the core ideas on my own** using Jupyter Notebooks to explore, experiment, and verify each step.

---

## About Micrograd

**Micrograd** is a minimalistic autograd engine that:
- Implements reverse-mode automatic differentiation
- Builds a dynamic computational graph
- Supports backpropagation for scalar-valued functions
- Forms the foundation of a simple neural network library

Despite its small size, micrograd captures the core ideas behind modern deep learning frameworks like **PyTorch** and **TensorFlow**.

---

## What I Learned

Through this project, I gained hands-on understanding of:

- Computational graphs and scalar operations
- Reverse-mode automatic differentiation
- Backpropagation from first principles
- How gradients flow through complex expressions
- How neural networks are built from basic building blocks
- The intuition behind modern deep learning frameworks

---

## Learning Resource

This project is inspired by and based on the following resource:

- **Andrej Karpathy – “The spelled-out intro to neural networks and backpropagation”**  
  YouTube: https://www.youtube.com/@AndrejKarpathy

All credit for the original idea and teaching goes to Andrej Karpathy.  
This repository is purely for **learning and educational purposes**.

This project helped bridge the gap between **theory and implementation**.

---

## ✨ Acknowledgements

- **Andrej Karpathy** — for the incredible explanation and inspiration
- The open-source ML community
